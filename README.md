# TAMU-Datathon-2020

Task 1: (Data crawling)
We have scraped through the data using Beautiful soup in python. We have initially got the product links for different categories (9 in total). Then, from these product links, we have scraped the product title, ratings, number of reviewers, most positive reviews and most negative reviews. Then we have used this info for the latent semantic analysis of the products. Then, we went further and extracted the outgoing product links from the webpage. For this, we had to switch the crawler from beautiful soup to selenium since soup cannot retrieve the dynamic content.

Task 2: (Latent Semantic Analysis)
So once we had the data about the products from all the categories, the first step is data processing. During the data processing step, we made sure to remove all the stop words, punctuation and non-ascii characters from the data. Once we cleaned the data, we generated the tf-idf vectors (Term Frequency - Inverse Document Frequency) for all the documents present in the dataset. Now, as the vocabulary space is really large, we had to cut down on the feature space that was generated by the tf-idf model. Therefore, for dimensionality reduction, we made use of Latent Semantic Analysis, which used SVD to map the original feature space into a low dimensional one. For this task, we considered the top 100 features. Then we used cosine similarity as a measure of relevance of the search query.

Task 3: (Clustering)
For the clustering part, once we calculate the cosine similarity between the query and all the documents, we consider the top 50 results based on the relevance scores. Now, in order to check the dominant category of these results, we applied KMeans clustering with the elbow method to determine the optimum number of clusters. Once we segregate all the results into clusters, we consider the cosine similarity between the feature vector of the query and the centroid of all the clusters and pick the cluster with the max relevance. We then calculated the relevance between the query and all the data points in the cluster to give the final results. Using this method the results were not up to the mark, but one interesting thing that we identified was that all the products with similar type were being placed in the same cluster. So, one improvement of this technique might be is, to apply clustering at the start of the process(i.e on the entire data) and then calculate the similarity scores only products present in the cluster which the query is most relevant to.

Task 4 (UI):
We have made a basic web application for the product search using Flask in python. The web application allows for a single product search query and retrieves the top 10 relevant results, with links to the original walmart products.
